<!DOCTYPE html>
<html lang="en"><head>
<script src="2025-04-28-workshop-ai-humanities_files/libs/clipboard/clipboard.min.js"></script>
<script src="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/tabby.min.js"></script>
<script src="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/popper.min.js"></script>
<script src="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="2025-04-28-workshop-ai-humanities_files/libs/quarto-html/quarto-syntax-highlighting-dark-4379b0ccadffce622b03caf4c46266b3.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.43">

  <meta name="author" content="Francesca Albrezzi falbrezzi@ucla.edu">
  <meta name="author" content="Ryan Horne rmhorne@ucla.edu">
  <title>Introduction to AI/ML/LLM for Humanities and Social Science</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/dist/theme/quarto-2ad2f6b6694a5336bcbecb8e357fdf46.css">
  <link href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#005c96" data-background-position="100% 100%" data-background-size="6%" class="quarto-title-block center">
  <h1 class="title">Introduction to AI/ML/LLM for Humanities and Social Science</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<b>Francesca Albrezzi</b><br> falbrezzi@ucla.edu 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<b>Ryan Horne</b><br> rmhorne@ucla.edu 
</div>
</div>
</div>

</section>
<section id="introductions" class="slide level2 smaller">
<h2>Introductions</h2>
<p>Hello! üëã</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Francesca Albrezzi, PhD</p>
<ul>
<li>Digital Research Manager and Specialist in OARC</li>
<li>Taught for DH and World Arts &amp; Cultures/Dance (WACD)</li>
<li>Editor-in-Chief and Director of the virtual gallery for the International Journal of Digital Art History</li>
<li>UC Women in Tech Committee Member</li>
<li>Expertise in Extended Reality (XR) Technologies</li>
</ul>
</div><div class="column" style="width:50%;">
<p>Ryan Horne, PhD</p>
<ul>
<li></li>
</ul>
</div></div>
</section>
<section id="agenda" class="slide level2">
<h2>Agenda</h2>
</section>
<section id="what-ai-is-good-at" class="slide level2">
<h2>What AI is Good At</h2>
<ul>
<li>Pattern recognition</li>
<li>Language processing</li>
<li>Image analysis</li>
</ul>
</section>
<section id="what-ai-is-not-good-at" class="slide level2">
<h2>What AI is NOT Good At</h2>
<ul>
<li>In-depth analysis</li>
<li>Contextual understanding</li>
<li>Nuanced interpretation</li>
</ul>
</section>
<section id="current-state-of-ai-ucla" class="slide level2">
<h2>Current State of AI @ UCLA</h2>
<ul>
<li><a href="https://adminvc.ucla.edu/blog/artificial-intelligence-faqs">AI FAQs</a></li>
<li><a href="https://adminvc.ucla.edu/people/chris-mattmann">Chris Mattmann - Chief Data and Artificial Intelligence Officer</a></li>
<li>[IT services has a page on ethical services of AI] (https://it.ucla.edu/news/new-resource-generative-ai-ucla)</li>
</ul>
</section>
<section id="classification-of-ai-systems" class="slide level2">
<h2>Classification of AI Systems</h2>
<div class="columns">
<div class="column" style="width:33%;">
<p><span class="section-1"><strong>Artificial Narrow Intelligence</strong></span></p>
<div>
<ul>
<li class="fragment">perform a narrow task with an assigned data set or<br>
</li>
<li class="fragment">operate within certain specified parameters<br>
</li>
<li class="fragment">Google Translate, Siri, and Alexa</li>
</ul>
</div>
</div><div class="column" style="width:33%;">
<p><strong>Artificial General Intelligence</strong>{.section-2}</p>
<ul>
<li>expected to learn and reason with without human intervention<br>
</li>
<li>same cognativbe level as humans<br>
</li>
</ul>
</div><div class="column" style="width:33%;">
<p><span class="section-3"><strong>Artificial Super Intelligence</strong></span></p>
<div>
<ul>
<li class="fragment">Far surpasses human abilities</li>
<li class="fragment">singularity</li>
</ul>
</div>
</div></div>
</section>
<section id="singulartiy" class="slide level2">
<h2>Singulartiy</h2>

<img data-src="images/singularity.jpeg" class="r-stretch"></section>
<section id="neural-network" class="slide level2">
<h2>Neural Network</h2>
<p>A neural network is a computational model inspired by the human brain, designed to recognize patterns and relationships in data.</p>
<ul>
<li>Made up of layers of nodes (neurons).</li>
<li>Each neuron processes inputs and passes the result to the next layer.</li>
<li>Learns by adjusting weights through training</li>
<li>Used For</li>
<li>Image recognition, natural language processing, speech recognition, and more.</li>
</ul>
</section>
<section id="neural-net" class="slide level2">
<h2>Neural Net</h2>

<img data-src="images/nn1.svg" class="r-stretch"></section>
<section id="deep-neural-net" class="slide level2">
<h2>Deep Neural Net</h2>

<img data-src="images/nn2.svg" class="r-stretch"></section>
<section id="definition-and-basic-concepts" class="slide level2">
<h2>Definition and Basic Concepts</h2>
<ul>
<li>Neural Networks, AI, and ML</li>
<li><a href="https://aiforhumanists.com/glossary/">Glossary</a></li>
</ul>
</section>
<section id="historical-development" class="slide level2">
<h2>Historical Development</h2>
<ul>
<li>Key milestones</li>
<li>Resources for further reading</li>
</ul>
</section>
<section id="importance-applications" class="slide level2">
<h2>Importance &amp; Applications</h2>
<ul>
<li>Digital Humanities and North Campus disciplines</li>
<li>Current research and use cases at UCLA</li>
</ul>
</section>
<section id="strengths-and-limitations" class="slide level2">
<h2>Strengths and Limitations</h2>
</section>
<section id="what-ai-is-good-at-1" class="slide level2">
<h2>What AI is Good At</h2>
<ul>
<li>Pattern recognition</li>
<li>Language processing</li>
<li>Image analysis</li>
</ul>
</section>
<section id="what-ai-is-not-good-at-1" class="slide level2">
<h2>What AI is NOT Good At</h2>
<ul>
<li>In-depth analysis</li>
<li>Contextual understanding</li>
<li>Nuanced interpretation</li>
</ul>
</section>
<section id="gofai-vs.-generative-ai" class="slide level2">
<h2>GoFAI vs.&nbsp;Generative AI</h2>
</section>
<section id="large-language-models-llms" class="slide level2">
<h2>Large Language Models (LLMs)</h2>
</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<ul>
<li>Key components (Transformers, Attention Mechanisms)</li>
<li>Training processes and dataset types</li>
</ul>
</section>
<section id="popular-llms" class="slide level2">
<h2>Popular LLMs</h2>
<ul>
<li>GPT-3, BERT</li>
</ul>
</section>
<section id="applications-in-digital-humanities" class="slide level2">
<h2>Applications in Digital Humanities</h2>
<ul>
<li>Text analysis, translation, content generation</li>
<li>Case Study: <a href="https://arxiv.org/abs/1912.01140">Literary Event Detection</a></li>
</ul>
</section>
<section id="how-llms-can-help" class="slide level2">
<h2>How LLMs Can Help</h2>
</section>
<section id="use-cases" class="slide level2">
<h2>Use Cases</h2>
<ul>
<li>Brainstorming and outlining</li>
<li>Prompt engineering for better AI responses</li>
</ul>
</section>
<section id="demos" class="slide level2">
<h2>Demos</h2>
<ul>
<li>Outline an AI presentation</li>
<li>AI-assisted code generation</li>
<li>Pleiades to Neo4j workflows</li>
<li>NLP Twitter / Bluesky data</li>
</ul>
</section>
<section id="issues" class="slide level2">
<h2>Issues</h2>
<ul>
<li>Generated essays &amp; ghost citations</li>
<li>Ethical concerns</li>
</ul>
</section>
<section id="large-vision-models-lvms-vision-language-models-vlms" class="slide level2">
<h2>Large Vision Models (LVMs) &amp; Vision Language Models (VLMs)</h2>
<p>Large Vision Models (LVMs) / Large Image Models: - AI systems designed to analyze visual data (images, videos). - Trained on vast datasets of images/videos. - Applications: Image classification, object detection, image segmentation, image generation, scene interpretation. Vision Language Models (VLMs): - Combine LVM capabilities with Natural Language Processing (NLP). - Applications: Creating text descriptions for images, answering questions about visual data, generating images from text prompts. Case Study: The Living Museum App https://www.livingmuseum.app/</p>
<p>::: {.notes} Large Vision Models (LVMs), also known as large image models, are AI systems that are designed to analyze visual data. Trained with large image or video datasets, LVMs are used in image classification, object detection, image segmentation, image generation, and scene interpretation. Another AI system that is applicable to visual resource research are Vision Language Models (VLMs), which combine NLP with LVMs to create textual descriptions for images, answer questions about visual data or create visual data based on text inputs. For example, Jonathan Talmi‚Äôs Living Museum app (https://www.livingmuseum.app/) uses Generative AI to allow users to have ‚Äúconversations‚Äù with the objects in the British Museum‚Äôs online collection (Museums + Heritage Advisor, 2024). In The Living Museum app (https://www.livingmuseum.app/), the model is not constrained to the British museum‚Äôs archival metadata or a specific curator‚Äôs comments, but draws on outside sources to respond to wide questions about culture, history, and artistic process and practice. If we are not aware of all the sources of information that the AI‚Äôs answers are being drawn from, how is the user expected to assess the validity of the information as well as weight contextual positioning of the information? Does authorship of information matter in this context? Why or why not? What issues arise when AI becomes the voice for a museum object? Pick an object and ask it questions. What can the AI ‚Äúsee‚Äù and what does it miss? :::</p>
</section>
<section id="how-ai-sees" class="slide level2">
<h2>How AI ‚ÄúSees‚Äù</h2>
<p>Example: Handwriting Analysis with AI - AI Model: e.g., Siamese Convolutional Neural Network (CNN). Process: - Feature Identification: Focuses on key elements of handwriting style. - Abstraction: Generates abstract numerical representations (feature vectors) for these significant elements. - Comparison: Measures the similarity or difference between these numerical representations from different handwriting samples (calculating the magnitude of difference). - Learning: Converts images into a numerical network, assigning varying values (weights) to different features, effectively learning to distinguish individual writing styles. Source: Du, William, Michael Fang and Margaret Shen. 2017. ‚ÄúSiamese Convolutional Neural Networks for Authorship Verification.‚Äù http://cs231n.stanford.edu/reports/2017/pdfs/801.pdf.</p>
<p>::: {.notes} But how does a computer ‚Äúsee‚Äù? Deep machine learning for visual material often operates by identifying the most important features or most uniquely characteristic features of the visual material. If you wanted to do a handwriting analysis, the AI network‚Äîe.g., a Siamese Convolutional Neural Network (CNN)‚Äîwould focus on identifying the most important features of handwriting, generating abstract features as a numerical representation of those elements it deemed significant. By essentially measuring the similarity or difference of those repeating features, the network can calculate the magnitude of differences between handwriting samples. Through a process of converting images to a numerical network, the model assigned varying values to different features, effectively learning to distinguish between individual writing styles (Du et. al, 2017). Akrish Adhikari recently presented here at UCLA on how determining whose penmanship is present in a particular document can determine authorship. In addition, there is a distinction between the act of thinking and the physical act of writing, which can underscore the significant labor of editors and transcriptionists, whose contributions profoundly shape our understanding of authorship. Marginalia is an important part of literary history and knowing who made what suggested edit can help us better understand the influence of thought on a particular piece of writing. :::</p>
</section>
<section id="handwriting-analysis-tool-hat" class="slide level2">
<h2>Handwriting Analysis Tool (HAT)</h2>
<p><a href="https://www.csmc.uni-hamburg.de/publications/software/hat.html">Handwriting Analysis Tool (HAT)</a> Case Study: Alexander Hamilton &amp; George Washington Papers - https://www.loc.gov/collections/george-washington-papers/about-this-collection/ - https://www.loc.gov/collections/alexander-hamilton-papers/about-this-collection/</p>
</section>
<section id="future-of-ai-ml-in-digital-humanities" class="slide level2">
<h2>Future of AI &amp; ML in Digital Humanities</h2>
</section>
<section id="fine-tuning-ai-for-humanities-research" class="slide level2">
<h2>Fine-Tuning AI for Humanities Research</h2>
<ul>
<li>Domain expertise in model refinement</li>
<li>Example: Human Pose Estimation (Bernasconi et al., 2023)</li>
</ul>
</section>
<section id="technical-barriers" class="slide level2">
<h2>Technical Barriers</h2>
<ul>
<li>AI-assisted tools like Microsoft‚Äôs Co-Pilot</li>
<li>Accessibility and skill development</li>
</ul>
</section>
<section id="open-vs.-closed-ai" class="slide level2 smaller">
<h2>Open vs.&nbsp;Closed AI</h2>
<div class="infogram-embed" data-id="a7ce073b-00d7-4dbc-bbdd-2821226d7326" data-type="interactive" data-title="Openness AI">

</div>
<script>!function(e,n,i,s){var d="InfogramEmbeds";var o=e.getElementsByTagName(n)[0];if(window[d]&&window[d].initialized)window[d].process&&window[d].process();else if(!e.getElementById(i)){var r=e.createElement(n);r.async=1,r.id=i,r.src=s,o.parentNode.insertBefore(r,o)}}(document,"script","infogram-async","https://e.infogram.com/js/dist/embed-loader-min.js");</script>
<ul>
<li>Innovation, accessibility, and security concerns</li>
<li><a href="https://ollama.com/">Ollama: Local AI</a></li>
<li>Retrieval-Augmented Generation (RAG)</li>
</ul>
<p>Luna, Angela. ‚ÄúThe Open or Closed AI Dilemma.‚Äù Bipartisan Policy Center Blog, May 2, 2024, <a href="https://bipartisanpolicy.org/blog/the-open-or-closed-ai-dilemma/">https://bipartisanpolicy.org/blog/the-open-or-closed-ai-dilemma/</a>. Accessed March 6, 2025.</p>
<p>South, Tobin, Jules Drean, Abhishek Singh, Guy Zyskind, Robert Mahari, Vivek Sharma, Praneeth Vepakomma, Lalana Kagal, Srinivas Devadas, and Alex Pentland. 2024. ‚ÄúA Roadmap for End-to-End Privacy and Security in Generative AI.‚Äù An MIT Exploration of Generative AI, September. <a href="https://doi.org/10.21428/e4baedd9.9af67664">https://doi.org/10.21428/e4baedd9.9af67664</a>.</p>
<aside class="notes">
<p>There are important ways an AI system can be opened or closed. We might think of it as a spectrum as this article and infovis by Angela Luna demonstrates (Luna 2024). The code for the model itself could be released open source where others can customize or contribute to it. Or it could be proprietary to a particular entity, meaning the code is only available to those with access and is protected under copyright. Models can be released for public use and still be closed, such as certain versions of ChatGPT. In these cases, users should not assume that any use of the model is private or protected in terms of their own intellectual property. Your data is being captured and processed on proprietary servers. Instead, you may choose to download and run an open-sourced LLM locally, using something like Ollama (https://ollama.com/), so that your data is not shared.</p>
<p>The AI system itself may also be designed to operate under more limited conditions. Imagine you just want to ask questions about a single set of documents and you only want answers that can be found directly in that data. AI knowledge retrieval systems, often using techniques like Retrieval-Augmented Generation (RAG), probe a single knowledge base (dataset) that the user defines, which increases output accuracy and contextual relevance. Rather than drawing on many different sources (e.g., a dataset that is akin to the whole of the internet), this approach limits the data scope and so the system can only respond based on the information that has been provided in the dataset. More secure systems are necessary when it comes to using AI/ML tools for sensitive or protected data (South et. al.&nbsp;2024).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-definitions" class="slide level2">
<h2>Key Definitions</h2>
<ul>
<li>Open AI (e.g., open-source models)</li>
<li>Closed AI (e.g., proprietary systems)</li>
</ul>
</section>
<section id="implications" class="slide level2">
<h2>Implications</h2>
<ul>
<li>Innovation, accessibility, and security concerns</li>
<li><a href="https://ollama.com/">Ollama: Local AI</a></li>
<li>Retrieval-Augmented Generation (RAG)</li>
</ul>
</section>
<section id="ethical-privacy-concerns" class="slide level2">
<h2>Ethical &amp; Privacy Concerns</h2>
<ul>
<li>AI decision-making in banking, housing, credit</li>
</ul>
</section>
<section id="ethical-concerns" class="slide level2">
<h2>Ethical Concerns</h2>
</section>
<section id="trust-authenticity" class="slide level2">
<h2>Trust &amp; Authenticity</h2>
<ul>
<li>AI‚Äôs role in misinformation/disinformation</li>
<li>Deepfakes &amp; synthetic media</li>
<li>Detection &amp; prevention strategies</li>
</ul>
</section>
<section id="bias-fairness" class="slide level2">
<h2>Bias &amp; Fairness</h2>
<ul>
<li>Algorithmic bias (Joy Buolamwini‚Äôs research)</li>
<li>Mitigation strategies &amp; ethical data collection</li>
</ul>
</section>
<section id="academic-integrity" class="slide level2">
<h2>Academic Integrity</h2>
<ul>
<li>AI-generated content accuracy</li>
<li>Copyright &amp; ownership debates</li>
</ul>
</section>
<section id="future-directions-trends" class="slide level2">
<h2>Future Directions &amp; Trends</h2>
</section>
<section id="emerging-technologies" class="slide level2">
<h2>Emerging Technologies</h2>
<ul>
<li>Advances in neural networks</li>
<li>AI‚Äôs evolving role in Digital Humanities</li>
</ul>
</section>
<section id="ethical-ai-at-ucla" class="slide level2">
<h2>Ethical AI at UCLA</h2>
<ul>
<li><a href="https://adminvc.ucla.edu/blog/artificial-intelligence-faqs">Chris Mattmann‚Äôs AI Resources</a></li>
<li><a href="https://it.ucla.edu/news/new-resource-generative-ai-ucla">Generative AI at UCLA</a></li>
</ul>
</section>
<section id="further-learning" class="slide level2">
<h2>Further Learning</h2>
<ul>
<li>Courses, articles, and bibliography</li>
</ul>
</section>
<section id="discussion-questions" class="slide level2">
<h2>Discussion &amp; Questions</h2>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="https://raw.githubusercontent.com/rmhorne/work-images/ed53108a33ec6274cff3dd8c06aad26405ef7645/images/creative-commons/cc-by-sa.svg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="2025-04-28-workshop-ai-humanities_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>